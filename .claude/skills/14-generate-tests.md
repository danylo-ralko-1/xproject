# Generate Tests for Feature Code

**Trigger:** "generate tests", "write tests", "create tests for story", "test this feature"

> **Standalone skill.** Run this AFTER developers have reviewed and edited the feature code generated by skill 12. Tests should be written against the finalized code, not the initial scaffold.

**Pre-checks:**
- Project name must be known (to read ADO credentials from project.yaml). If not: "Which project should I use?"
- ADO must be connected. If not: "I need ADO credentials configured."
- User must provide a **Story ID** — the ADO work item number. If not: "Which story should I generate tests for? Give me the ADO ID (e.g., #752)."
- User must provide the **target codebase path**. If not: "Where is your product codebase?"
- The feature branch must exist and contain the developer-edited code. Check: `git branch --list 'feature/*<story-id>*'`. If not: "I can't find a feature branch for story #XXX. Has the code been generated and reviewed?"

---

## Step 1: Gather Inputs

Collect from the user (ask for anything missing):

1. **Story ID** — which ADO story to test
2. **Target codebase path** — absolute path to the product repo
3. **Feature branch name** (optional) — auto-detect from story ID if not provided

## Step 2: Fetch Story from ADO

Fetch the story with full details (same as skill 12, Step 2):

```bash
cd ~/Downloads/xproject && python3 -c "
from core.config import load_project
from core.ado import from_project, get_work_items_by_query
import json
p = load_project('<ProjectName>')
c = from_project(p)
wiql = 'SELECT [System.Id] FROM WorkItems WHERE [System.Id] = <StoryID>'
items = get_work_items_by_query(c, wiql)
print(json.dumps(items, indent=2, default=str))
"
```

Extract:
- **Acceptance Criteria** — each AC group maps to a test suite
- **Technical Context block** — data model → type assertions, states → render tests, interactions → user event tests, API hints → mock definitions

## Step 3: Read the Feature Code

Switch to the feature branch in the target codebase:

```bash
cd <target_codebase_path>
git checkout feature/<story-id>-<short-name>
```

Read ALL feature files — the developer-edited versions, not the original scaffold:

### 3a: Identify feature files

Read `FEATURE.md` to get the list of generated files. Then read each one. Also check for any files the developer added that aren't in FEATURE.md.

### 3b: Understand what the code actually does

For each component/module, note:
- **Props and their types** — these become test inputs
- **State variables and transitions** — these become state assertion tests
- **Event handlers** — these become user interaction tests
- **API calls** — these become mock/integration tests
- **Conditional rendering** — these become render variant tests (loading, error, empty, etc.)
- **Validation logic** — these become validation edge case tests

### 3c: Check for developer changes

Compare the current code to what was originally generated. The developer may have:
- Renamed components or files
- Changed API call signatures
- Added new state management
- Modified validation rules
- Added new sub-components

**Test what exists now, not what was originally generated.**

## Step 4: Analyze Existing Test Patterns

**First**, check if `projects/<ProjectName>/codebase-patterns.md` exists. If it does:
- Read the **Testing Patterns** section (Category 11) — it contains actual test snippets showing the project's exact conventions: test file naming, location, imports, render helpers, mock patterns, assertion style, describe/it nesting
- Also read the **Mock patterns** subsection for how the project mocks APIs and modules
- **Staleness check:** Read the `**Commit:**` line and run `git rev-list <scan-hash>..HEAD --count`. If >30 commits, warn: "The codebase patterns file was scanned [N] commits ago. Test conventions may have changed. Want me to re-scan before generating tests?"
- Follow these patterns exactly — same imports, same helpers, same structure

**If no `codebase-patterns.md` exists**, fall back to manual analysis. Study how the codebase tests things:

- **Test framework:** Jest / Vitest / Mocha / Playwright / Cypress
- **Test file location:** co-located (`Component.test.tsx`) or separate (`__tests__/Component.test.tsx`)
- **Test file naming:** `.test.ts`, `.spec.ts`, `_test.go`, etc.
- **Rendering library:** React Testing Library / Enzyme / Vue Test Utils
- **Mock patterns:** jest.mock / vi.mock / MSW / manual mocks
- **Assertion style:** expect().toBe() / assert() / should()
- **Test structure:** describe/it nesting conventions, setup/teardown patterns

Read 2-3 existing test files to match the exact style.

## Step 5: Generate Tests

Generate comprehensive tests that map to both the AC and the actual code implementation.

### Test structure (per component/module):

```
describe('[ComponentName]', () => {
  // Setup: render helpers, mock data, common props

  describe('AC 1: [AC Title]', () => {
    it('[specific testable behavior from AC bullet 1]', ...)
    it('[specific testable behavior from AC bullet 2]', ...)
  })

  describe('AC 2: [AC Title]', () => {
    it('[specific testable behavior]', ...)
  })

  describe('State management', () => {
    it('renders loading state', ...)
    it('renders error state with retry', ...)
    it('renders empty state', ...)
  })

  describe('User interactions', () => {
    it('[interaction from Technical Context]', ...)
  })
})
```

### What to test:

**From the Acceptance Criteria:**
- Each AC bullet point → at least one test case
- Group tests by AC group using nested `describe` blocks
- Test names should read as behavior specifications: "filters results when user types in search field"

**From the Technical Context block:**
- **Data Model** → test that components accept and render the correct data shape, test type validation
- **States** → test each UI state renders correctly (default, loading, empty, error, success)
- **Interactions** → test each event chain: click → handler → state change → re-render
- **API Hints** → mock each endpoint, test success and error responses

**From the actual code:**
- Conditional rendering branches → test each branch
- Form validation → test valid input, invalid input, boundary values
- Error boundaries → test error recovery
- Accessibility → basic a11y checks (roles, labels, keyboard navigation)

### Test quality rules:

1. **Match existing test style exactly** — same imports, same helpers, same assertion patterns
2. **Use the project's existing test utilities** — if there's a `renderWithProviders` helper, use it
3. **Mock at the right level** — mock API calls, not internal functions. Use MSW if the project uses it.
4. **Test behavior, not implementation** — test what the user sees and does, not internal state names
5. **Each test should be independent** — no test should depend on another test's outcome
6. **Include meaningful test data** — use realistic values that match the data model, not `"test"` and `123`
7. **Cover the sad path** — every success test should have a corresponding failure test
8. **NO snapshot tests** unless the codebase already uses them extensively

### What NOT to test:

- Third-party library internals (don't test that React renders, test what YOUR component renders)
- Styling (don't assert CSS classes unless they drive behavior)
- Implementation details (don't test state variable names, test visible outcomes)

## Step 6: Generate API/Integration Tests (if applicable)

If the story has backend work and the backend code exists on the branch:

- Generate API endpoint tests for each endpoint in API-CONTRACT.md
- Test request validation (required fields, format, constraints)
- Test success responses (correct shape, correct data)
- Test error responses (401, 403, 404, 409, 422)
- Test business logic rules from the Server-Side Business Logic section
- Test authorization rules (correct role → success, wrong role → 403)

## Step 7: Present Test Plan for Approval

Before writing any files, show the user:

```
Test plan for Story #XXX: [Title]

Files to create:
- src/components/Feature/FeatureName.test.tsx (12 tests)
- src/hooks/useFeatureName.test.ts (6 tests)
- src/components/Feature/SubComponent.test.tsx (4 tests)

Coverage by AC:
- AC 1: Search behavior — 4 tests (search input, debounce, results update, no results)
- AC 2: Filter controls — 3 tests (mode toggle, dropdown filter, combined filters)
- AC 3: Table display — 3 tests (letter grouping, expandable rows, column sorting)
- AC 4: Loading/error — 2 tests (loading skeleton, error banner with retry)

Additional tests:
- API mocking: 3 tests (fetch success, fetch error, pagination)
- Accessibility: 2 tests (keyboard nav, screen reader labels)

Total: 22 tests
```

**WAIT for user approval before writing files.**

## Step 8: Write Test Files

On approval, write the test files to the feature branch. Place them according to the codebase convention (co-located or in `__tests__/`).

## Step 9: Run Tests

Run the test suite to verify tests pass:

```bash
cd <target_codebase_path>
# Use the project's test command
npm test -- --filter <feature-name>
# or
npx vitest run <path-to-tests>
# or whatever the project uses
```

If tests fail:
- **Fix tests that fail due to test bugs** (wrong selector, missing mock, incorrect assertion)
- **Do NOT modify the feature code** — tests must adapt to the code, not the other way around
- **Flag tests that fail because the code doesn't implement an AC** — report to the user: "AC 3 bullet 2 says X, but the code doesn't implement this. Should I skip this test or is this a bug?"

## Step 10: Commit and Report

**Show results before committing:**

```
Test results for Story #XXX:
- 20/22 tests passing
- 2 tests skipped (AC not yet implemented — see below)

Skipped:
- "filters by Associated Data Set dropdown" — dropdown not connected to API yet (TODO in code)
- "keyboard navigation through alphabet bar" — keyboard handler not implemented

Files created:
- src/components/Glossary/GlossaryPage.test.tsx
- src/hooks/useGlossaryTerms.test.ts
```

Ask: "Ready to commit these tests to the feature branch?"

On approval:

```bash
cd <target_codebase_path>
git add <test-files>
git commit -m "$(cat <<'EOF'
test(#<StoryID>): add tests for <feature-name>

Generated comprehensive tests from ADO acceptance criteria.
See test plan in commit for coverage details.
XX tests passing, Y skipped (AC not yet implemented).

Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>
EOF
)"
git push
```

## Step 11: Update ADO

Update the story in ADO:
- **Append to FEATURE.md** (or to the story's Description): a note that tests have been generated with count and coverage summary
- **Tag:** `Claude Modified Story` (preserve existing tags)

## Step 12: Next Steps

Tell the user:
"Tests are committed to the feature branch. You can:
1. **Review the tests** — check that they match your expectations
2. **Run tests in CI** — push triggers your CI pipeline
3. **Generate tests for another story** — give me the next story ID
4. **Handle a change request** — if scope changes, I can update both code and tests"
